{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20b140",
   "metadata": {},
   "source": [
    "# Satellite Image Change Detection Demo\n",
    "\n",
    "This notebook demonstrates the usage of our satellite image change detection system. We'll go through the following steps:\n",
    "\n",
    "1. Setup and environment preparation\n",
    "2. Data loading and preprocessing\n",
    "3. Model training (small demo)\n",
    "4. Running inference on test images\n",
    "5. Visualizing and evaluating results\n",
    "6. Post-processing for output refinement\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6593ad",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Preparation\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e918a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the project root to path to import our modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import our modules\n",
    "from changedetect.src.models.siamese_unet import get_change_detection_model\n",
    "from changedetect.src.data.dataset import create_dataloaders, create_test_dataloader\n",
    "from changedetect.src.data.preprocess import preprocess_pair\n",
    "from changedetect.src.utils.metrics import calculate_all_metrics\n",
    "from changedetect.src.utils.postprocessing import process_change_detection_mask\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd7032",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Now, let's load some sample data and preprocess it. If you don't have any sample data, you can download it using our data download module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to download data (commented out)\n",
    "# from changedetect.src.data.download import download_sentinel2_copernicus\n",
    "# download_sentinel2_copernicus(\n",
    "#     output_dir=str(DATA_DIR / \"raw\"),\n",
    "#     lat=28.1740, lon=77.6126,  # Urban area example\n",
    "#     date_range=(\"2023-01-01\", \"2023-02-01\"),\n",
    "#     cloud_cover_max=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, let's use a pre-downloaded image pair\n",
    "# Replace with your actual image paths or use sample data\n",
    "image_t1_path = DATA_DIR / \"samples\" / \"urban_t1.tif\"\n",
    "image_t2_path = DATA_DIR / \"samples\" / \"urban_t2.tif\"\n",
    "mask_path = DATA_DIR / \"samples\" / \"urban_mask.tif\"\n",
    "\n",
    "# Check if files exist\n",
    "if not image_t1_path.exists() or not image_t2_path.exists():\n",
    "    print(\"Sample files not found. Please add sample data or modify the paths.\")\n",
    "else:\n",
    "    # Display the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    with rasterio.open(image_t1_path) as src:\n",
    "        img_t1 = src.read()\n",
    "        show(img_t1, ax=axs[0])\n",
    "        axs[0].set_title(\"Time 1 Image\")\n",
    "    \n",
    "    with rasterio.open(image_t2_path) as src:\n",
    "        img_t2 = src.read()\n",
    "        show(img_t2, ax=axs[1])\n",
    "        axs[1].set_title(\"Time 2 Image\")\n",
    "    \n",
    "    if mask_path.exists():\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1)\n",
    "            axs[2].imshow(mask, cmap='viridis')\n",
    "            axs[2].set_title(\"Ground Truth Change Mask\")\n",
    "    else:\n",
    "        axs[2].set_title(\"No Ground Truth Available\")\n",
    "        axs[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda3ed9",
   "metadata": {},
   "source": [
    "### Preprocessing the Images\n",
    "\n",
    "Now, let's preprocess the images before feeding them to the model. This includes co-registration, normalization, and other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "if image_t1_path.exists() and image_t2_path.exists():\n",
    "    with rasterio.open(image_t1_path) as src:\n",
    "        img_t1 = src.read()\n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(image_t2_path) as src:\n",
    "        img_t2 = src.read()\n",
    "    \n",
    "    # Preprocess image pair\n",
    "    preprocessed_t1, preprocessed_t2 = preprocess_pair(img_t1, img_t2)\n",
    "    \n",
    "    # Visualize preprocessed images\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    # Original images (first 3 bands for RGB)\n",
    "    rgb_t1 = np.transpose(img_t1[:3], (1, 2, 0))\n",
    "    rgb_t2 = np.transpose(img_t2[:3], (1, 2, 0))\n",
    "    \n",
    "    # Normalize for display\n",
    "    def normalize_for_display(img):\n",
    "        img = img.copy()\n",
    "        for i in range(img.shape[2]):\n",
    "            img[:,:,i] = (img[:,:,i] - img[:,:,i].min()) / (img[:,:,i].max() - img[:,:,i].min() + 1e-8)\n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    rgb_t1 = normalize_for_display(rgb_t1)\n",
    "    rgb_t2 = normalize_for_display(rgb_t2)\n",
    "    \n",
    "    axs[0, 0].imshow(rgb_t1)\n",
    "    axs[0, 0].set_title(\"Original Time 1\")\n",
    "    \n",
    "    axs[0, 1].imshow(rgb_t2)\n",
    "    axs[0, 1].set_title(\"Original Time 2\")\n",
    "    \n",
    "    # Preprocessed images\n",
    "    pre_rgb_t1 = np.transpose(preprocessed_t1[:3], (1, 2, 0))\n",
    "    pre_rgb_t2 = np.transpose(preprocessed_t2[:3], (1, 2, 0))\n",
    "    \n",
    "    pre_rgb_t1 = normalize_for_display(pre_rgb_t1)\n",
    "    pre_rgb_t2 = normalize_for_display(pre_rgb_t2)\n",
    "    \n",
    "    axs[1, 0].imshow(pre_rgb_t1)\n",
    "    axs[1, 0].set_title(\"Preprocessed Time 1\")\n",
    "    \n",
    "    axs[1, 1].imshow(pre_rgb_t2)\n",
    "    axs[1, 1].set_title(\"Preprocessed Time 2\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2efc94",
   "metadata": {},
   "source": [
    "## 3. Model Training (Small Demo)\n",
    "\n",
    "For demonstration purposes, let's create a small training loop to show how the model is trained. In practice, you would use our training script with more data and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17268d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = get_change_detection_model(\n",
    "    model_type='siamese_unet',\n",
    "    in_channels=3,  # Using RGB channels for demo\n",
    "    out_channels=1,  # Binary change detection\n",
    "    features=32,    # Reduced features for demo\n",
    "    bilinear=True,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c367049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders (you need a proper dataset for this)\n",
    "# For demo, we'll just show the code without executing it\n",
    "\n",
    "'''\n",
    "# Define paths\n",
    "image_dir = DATA_DIR / \"train\" / \"images\"\n",
    "mask_dir = DATA_DIR / \"train\" / \"masks\"\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    image_pairs_dir=str(image_dir),\n",
    "    mask_dir=str(mask_dir),\n",
    "    tile_size=256,\n",
    "    batch_size=4,  # Small batch size for demo\n",
    "    val_split=0.2,\n",
    "    num_workers=2,\n",
    "    overlap=32\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Few epochs for demo\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Get data\n",
    "        t1_images = batch['t1'].to(device).float()\n",
    "        t2_images = batch['t2'].to(device).float()\n",
    "        masks = batch['mask'].to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(t1_images, t2_images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % 2 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get data\n",
    "                t1_images = batch['t1'].to(device).float()\n",
    "                t2_images = batch['t2'].to(device).float()\n",
    "                masks = batch['mask'].to(device).float()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(t1_images, t2_images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_save_path = OUTPUT_DIR / \"models\" / \"demo_model.pth\"\n",
    "model_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59b11c",
   "metadata": {},
   "source": [
    "## 4. Running Inference on Test Images\n",
    "\n",
    "Now, let's run inference on a test image pair using a pre-trained model. If you don't have a pre-trained model, you can use the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume we have a pre-trained model available\n",
    "# In practice, you would load this from a file\n",
    "\n",
    "# For demonstration, we'll use the model we created earlier\n",
    "# In a real scenario, you would load a trained model like this:\n",
    "\n",
    "'''\n",
    "# Load pre-trained model\n",
    "model_path = OUTPUT_DIR / \"models\" / \"best_model.pth\"\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "'''\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on our sample images\n",
    "if image_t1_path.exists() and image_t2_path.exists():\n",
    "    # Read images again if needed\n",
    "    with rasterio.open(image_t1_path) as src:\n",
    "        img_t1 = src.read()\n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(image_t2_path) as src:\n",
    "        img_t2 = src.read()\n",
    "    \n",
    "    # Preprocess images\n",
    "    preprocessed_t1, preprocessed_t2 = preprocess_pair(img_t1, img_t2)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    t1_tensor = torch.from_numpy(preprocessed_t1).unsqueeze(0).to(device).float()\n",
    "    t2_tensor = torch.from_numpy(preprocessed_t2).unsqueeze(0).to(device).float()\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(t1_tensor, t2_tensor)\n",
    "        prediction = torch.sigmoid(output) > 0.5\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    prediction = prediction.squeeze().cpu().numpy().astype(np.uint8) * 255\n",
    "    \n",
    "    # Save prediction\n",
    "    output_dir = OUTPUT_DIR / \"predictions\"\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    output_path = output_dir / \"demo_prediction.tif\"\n",
    "    \n",
    "    # Update profile for output\n",
    "    out_profile = profile.copy()\n",
    "    out_profile.update({\n",
    "        'count': 1,\n",
    "        'dtype': 'uint8',\n",
    "        'compress': 'lzw',\n",
    "        'nodata': 0\n",
    "    })\n",
    "    \n",
    "    # Write prediction to file\n",
    "    with rasterio.open(output_path, 'w', **out_profile) as dst:\n",
    "        dst.write(prediction, 1)\n",
    "    \n",
    "    print(f\"Prediction saved to {output_path}\")\n",
    "    \n",
    "    # Display results\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display original images (RGB bands)\n",
    "    rgb_t1 = np.transpose(preprocessed_t1[:3], (1, 2, 0))\n",
    "    rgb_t2 = np.transpose(preprocessed_t2[:3], (1, 2, 0))\n",
    "    \n",
    "    # Normalize for display\n",
    "    rgb_t1 = normalize_for_display(rgb_t1)\n",
    "    rgb_t2 = normalize_for_display(rgb_t2)\n",
    "    \n",
    "    axs[0].imshow(rgb_t1)\n",
    "    axs[0].set_title(\"Time 1 Image\")\n",
    "    \n",
    "    axs[1].imshow(rgb_t2)\n",
    "    axs[1].set_title(\"Time 2 Image\")\n",
    "    \n",
    "    # Display prediction\n",
    "    axs[2].imshow(prediction, cmap='viridis')\n",
    "    axs[2].set_title(\"Predicted Change Mask\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35d9a2",
   "metadata": {},
   "source": [
    "## 5. Visualizing and Evaluating Results\n",
    "\n",
    "Let's compare the prediction with the ground truth and calculate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad18263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth if available\n",
    "if mask_path.exists() and 'prediction' in locals():\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        gt_mask = src.read(1)\n",
    "        gt_mask_binary = (gt_mask > 0).astype(bool)\n",
    "    \n",
    "    # Convert prediction to binary mask\n",
    "    pred_binary = (prediction > 0).astype(bool)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_all_metrics(pred_binary, gt_mask_binary)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Evaluation Metrics:\")\n",
    "    print(f\"IoU: {metrics['iou']:.4f}\")\n",
    "    print(f\"Dice (F1): {metrics['dice']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axs[0].imshow(gt_mask_binary, cmap='viridis')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    \n",
    "    axs[1].imshow(pred_binary, cmap='viridis')\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    \n",
    "    # Confusion visualization\n",
    "    # - True positive (green)\n",
    "    # - False positive (red)\n",
    "    # - False negative (blue)\n",
    "    # - True negative (black)\n",
    "    confusion = np.zeros((*pred_binary.shape, 3), dtype=np.uint8)\n",
    "    \n",
    "    # True positive: Prediction = 1, Ground Truth = 1\n",
    "    confusion[pred_binary & gt_mask_binary] = [0, 255, 0]  # Green\n",
    "    \n",
    "    # False positive: Prediction = 1, Ground Truth = 0\n",
    "    confusion[pred_binary & ~gt_mask_binary] = [255, 0, 0]  # Red\n",
    "    \n",
    "    # False negative: Prediction = 0, Ground Truth = 1\n",
    "    confusion[~pred_binary & gt_mask_binary] = [0, 0, 255]  # Blue\n",
    "    \n",
    "    axs[2].imshow(confusion)\n",
    "    axs[2].set_title(\"Confusion (Green=TP, Red=FP, Blue=FN)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c3e6d",
   "metadata": {},
   "source": [
    "## 6. Post-processing for Output Refinement\n",
    "\n",
    "Finally, let's apply post-processing to refine the predicted mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8539896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post-processing\n",
    "if 'prediction' in locals():\n",
    "    # Save the raw prediction to a temporary file\n",
    "    raw_pred_path = output_dir / \"raw_prediction.tif\"\n",
    "    \n",
    "    with rasterio.open(raw_pred_path, 'w', **out_profile) as dst:\n",
    "        dst.write(prediction, 1)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_path = output_dir / \"processed_prediction.tif\"\n",
    "    \n",
    "    process_change_detection_mask(\n",
    "        str(raw_pred_path),\n",
    "        str(processed_path),\n",
    "        min_size=10,\n",
    "        apply_opening=True,\n",
    "        apply_closing=True,\n",
    "        fill_holes=True\n",
    "    )\n",
    "    \n",
    "    # Read the processed mask\n",
    "    with rasterio.open(processed_path) as src:\n",
    "        processed_mask = src.read(1)\n",
    "    \n",
    "    # Compare raw and processed masks\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    axs[0].imshow(prediction, cmap='viridis')\n",
    "    axs[0].set_title(\"Raw Prediction\")\n",
    "    \n",
    "    axs[1].imshow(processed_mask, cmap='viridis')\n",
    "    axs[1].set_title(\"Processed Prediction\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics for the processed mask\n",
    "    if mask_path.exists():\n",
    "        processed_binary = (processed_mask > 0).astype(bool)\n",
    "        processed_metrics = calculate_all_metrics(processed_binary, gt_mask_binary)\n",
    "        \n",
    "        print(f\"\\nProcessed Mask Metrics:\")\n",
    "        print(f\"IoU: {processed_metrics['iou']:.4f}\")\n",
    "        print(f\"Dice (F1): {processed_metrics['dice']:.4f}\")\n",
    "        print(f\"Precision: {processed_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {processed_metrics['recall']:.4f}\")\n",
    "        print(f\"Accuracy: {processed_metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e81ed",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use our satellite image change detection system. We covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Loading and preprocessing data\n",
    "3. Training a model (demo code)\n",
    "4. Running inference on test images\n",
    "5. Visualizing and evaluating results\n",
    "6. Applying post-processing for output refinement\n",
    "\n",
    "This system can be used for detecting various types of changes in satellite imagery, such as urban expansion, deforestation, construction activities, and more. For production use, you would typically train on larger datasets and for more epochs to achieve better performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
