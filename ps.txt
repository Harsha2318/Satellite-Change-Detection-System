PS-10: Change Detection using Satellite Imageries 
1. General Description 
Availability of every day satellite imageries, giving carpet coverage of entire landmass 
of regions of our interest, has given the capability to continuously monitor any new 
developments. But it is impossible to handle this much volume of satellite imageries 
using trained human resource to analyse because of the scale and volume. Hence 
automation of change detection on such datasets using AI/ML has great operational 
usage to detect any new man-made developments including roads/tracks and other 
facilities of interest. AI/ML based change detection models can reduce false alarms in 
such change detection. 
2. Problem Statement 
a. Compare two satellite imageries of same sensor, with same location of 
different time periods. 
b. Extract the man-made changes as a change mask in both georeferenced raster 
format, where pixel value 1 represents change and pixel value 0 represents no 
change, and vector format. 
3. Outcomes 
a. Stage-1 
1. Results to be shown as both raster change mask and corresponding vector 
file, extracted from the satellite imagery pair. 
2. Output file format is explained in evaluation methodology section 8. 
b. Stage-2:  Stage-2 problem will include change detection and further 
classification of detected change. However, solutions expected need to be 
developed for both Electro-Optical imagery pair and SAR imagery pair. Exact 
details will be updated to Stage-1 selected teams/participants.  
c. Stage-3: Stage-3 problem will remain as change detection and classification, 
however, exact details will be updated to Stage-2 selected teams/participants  
4. Datasets 
Stage Type of Dataset 
Format Objects /features to 
be searched 
Stage
1 
 ResourceSat-2 
(LISS-IV sensor) at 
5.8m resolution 
Tiff/jp2 
file.  
Remarks 
Man-made change 
detection on Electro
optical (EO) imagery. 
 Sentinel-2 at 10m 
resolution 
Stage
2 
 ResourceSat-2 
(LISS-IV sensor) at 
5.8m resolution 
Data to be 
downloaded from 
Bhoonidhi and 
Copernicus Portal. 
(*Refer links 
below the table) 
Tiff/jp2 
file. 
 Sentinel-2 at 10m 
resolution 
Man-made change 
detection and 
classification on EO 
and Synthetic 
Aperture Radar 
(SAR) imagery.  
Source for 
downloading 3 m 
multispectral 
satellite imagery 
will be shared 
1 
 Multispectral 
satellite imagery at 
3m resolution 
 Sentinel-1 (SAR) 
Stage
3 
 Internal Data 
 Both SAR AND EO  
with Stage-2 
participants.  
Classes may include  
1. Kacha Tracks 
2. Buildings  
3. Roads 
4. Land 
clearing/construction 
activities 
5. Large-scale infra 
development (e.g., 
dams, solar, mining, 
factories, warehouse) 
6. New Village 
Settlements 
Tiff/jp2 
file. 
Man-made change 
detection and 
classification on both 
EO and SAR imagery 
Data Source will 
be shared with 
Stage-3 
participants 
*Data Source for Stage-1 
Bhoonidhi Portal: https://bhoonidhi.nrsc.gov.in/bhoonidhi/home.html 
Copernicus Portal: https://browser.dataspace.copernicus.eu 
5. Dataset Arrangement for Stage-1 
a. Training Dataset: For development of the solution, Participants are expected to 
download and use data of the above-mentioned satellite sensors for stage-1. 
b. Mock Dataset: Mock sets have been identified for the participants to test their 
solution.  Details of the Mock set will be released on T0 + 45 day i.e. 15th Sep 2025. 
Solutions can be submitted on every Thursday from week commencing 15th Sep 
2025. Participants can submit their results generated on Mock Set. The results 
submitted on this set will not be used by the organiser to short-list the participants 
for offline evaluation, but is meant for self-assessment and improvement of the 
developed solutions by the participants.  Performance of the solution’ results will 
be ranked and displayed on leader-board by Tuesday of corresponding week 
commencing 15th Sep 2025.  Ranking will be based on the comparison between 
submitted results and ground truth.  
c. Shortlisting Dataset (Online): Details of the shortlisting dataset, containing list 
of 4 pairs of satellite imageries (02 each from   both sensors, LISS-4 & Sentinel-2 
hence makes data size approx. 10 GB), will be released on 31st Oct 2025 @ 1200 
Hrs for download and inference generation. Participants have to submit the 
results of their solution on this set for the Stage-1 evaluation till 1600 Hrs of the 
same day i.e. 31st Oct 2025. They will be shortlisted based on evaluation of their 
submitted results, against this Shortlisting Dataset only. Hence, it is mandatory for 
participants to timely submit their results.  
d.  Holdout Dataset(Offline): A holdout dataset will be given to the participants, 
called for offline evaluation, to show their solution after Stage-1 deadline i.e. post 
31st Oct 2025. The offline evaluation timeline will be shared later to the shortlisted 
participants. 
2 
3 
 
e. Sample Set: sample_set.zip. This set contains 02 sample image-pairs with 
corresponding man-made change mask in raster and vector format, intended to 
serve as reference. 
 
6. Guidelines for Stage-1 Challenge 
i. Participants are expected to develop a solution that will work on different type 
of terrain features (sample locations provided in the list below). The desired 
solution should effectively detect man-made changes, in an automated way, for 
these different terrain conditions. 
S.No Terrain Latitude 
(N) 
Longitude 
(E) 
1 Snow 34.0531 74.3909 
2 Plain 13.3143 77.6157 
3 Hill 31.2834 76.7904 
4 Desert 26.9027 70.9543 
5 Forest 23.7380 84.2129 
6 Urban 28.1740 77.6126 
 
ii. Participants are expected to download and use data of the satellite sensors 
mentioned in datasets section, for development. They are free to supplement 
it with other data sources. 
iii. Participants are expected to develop an AI/ML Model which can identify man
made changes from given image pairs. 
iv. Participants are free to use any development framework/ language. 
v. When two images are being compared by the model, it should give a mask of 
man-made changes in georeferenced raster format, where pixel value 1 
represents change and pixel value 0 represents no change, along with a 
corresponding vector file of the results.   
vi. The generated output mask should be georeferenced in GeoTIFF format 
(raster) and shapefile (vector). 
vii. Specifications of the satellite imageries used here (Maximum size) are 
mentioned below for reference. 
a. Sentinel-2: (~11000 x 11000 pixels at 10m resolution) 
b. Resourcesat-2: (~18000 x 17000 pixels at 5.8m resolution) 
viii. Evaluation of the solutions will be held in both online and offline mode. 
ix. Participants can submit the results generated on Mock Dataset on every 
Thursday from week commencing 15th Sep 2025. Using a leader-board, 
Ranking - based on results submitted - of the participants will be shared by 
Tuesday of corresponding week commencing 15th Sep 2025. This ranking is 
only for indicating the improvement requirement in the solution for 
participants only.  
x. Selection of 15-20 participants for offline-evalution in Stage-1: On 31st Oct 
2025, the details of Shortlisting Dataset will be made available on this website. 
Results generated on the Shortlisting Dataset imageries will be evaluated for 
final selection of  15-20 participants. The number may vary based on the 
overall performance at the discretion of the Jury for this Problem Statement. 
Along with results, hash value (using md5 algorithm) of the solution needs to 
be submitted by the participant on 31st Oct 2025. Submissions found 
Incomplete in any manner will not be considered for further processing. The 
shortlisted participants will be published alongwith the cutoff score as per the 
evaluation criteria. Participants individual scores will be shared over the 
email. 
xi. 
During offline evaluation, the model will be verified using this Hash value, 
subsequent upon which further evaluation will be carried out. Any kind of 
unfair means be avoided while developing and generating the solution and 
results, failing which will leads to cancellation of participation for the grand 
challenge and organisers can call the next participant from leader-board for 
evaluation.    
7. Sessions with Mentors\Experts 
a. For Stage-1, the organisers plan to meet participants via online meet or email 
to resolve their doubts, if any. This provision will be made active from 15th 
Aug 2025 and details regarding interaction will be shared on this 
website.  Kindly keep viewing this website regularly for updates on this. 
b. There will be sessions with Mentors\Experts in Stage-2 and Stage-3 for the 
willing selected participants to help them in achieving the best solutions 
. 
8. Evaluation Methodology for Stage-1 
A. Online Solution Evaluation during Stage-1 
a. General Instructions 
1. Participants will be given Mock Dataset details which they will download. They 
can use this dataset to test their solution performance. Solutions are expected 
to be submitted on every Thursday from week commencing 15th Sep 2025 and 
ranking will be shared by Tuesday of corresponding week commencing 15th 
Sep 2025. A leader board will be displayed for assessment and improvement 
of solution, based on test dataset. The submitted results against Mock Dataset 
will not be used for selection of participants by Organiser but to give the 
assessment of their solution performance. 
2. The results submitted against Mock Dataset will be evaluated against the 
ground truth, withheld by the organizer, to score their performance. 
Scores/Rankings will be computed based on the evaluation metrics indicated 
below:  
Category 
Criteria 
Description 
Metric 
Evaluation 
Jaccard Index Score based on official metric on 
Test Set satellite imagery 
4 
3. On 31st October 2025, details of the Shortlisting Dataset (online), listing scene 
ids, will be made available on website @ 1200 Hrs. Participants need to 
download the dataset and generate the results (in raster and vector format) 
which will be submitted on the website, along with their model hash value 
(using md5 algorithm). The website submission will remain open from 1200 
hrs and participants can submit their outputs till 1600 hrs. So please ensure 
this submission by 1600h on 31st Oct 2025. 
4. The generated georeferenced change mask in raster (GeoTIFF) and vector 
(shapefile) format, for each image pair, need to be submitted for evaluation.  
5. Based on the performance on Shortlisting Dataset (online), top 15-20 
participants will be called for offline evaluation. 
b. Input 
The solution developed must take following inputs: 
1. Image-pairs (Two images of same sensor of same location, with varying 
time period) 
c. Output 
The solution developed must: 
1. Compare the two images for man-made changes. 
2. Output a raster mask in GeoTIFF format where pixel value 1 represents 
change and pixel value 0 represents no change. 
3. Corresponding vector file in shapefile format need to be generated.  
4. Submission of Results: Participants need to generate and submit the 
results for all image pairs listed in ‘Mock’ and ‘Shortlisting Dataset’. Name 
each 
image-pair 
change 
mask 
output 
file 
name as 
‘Change_Mask_Lat_Long.tif’ (Raster File) and ‘Change_Mask_Lat_Long.shp’ 
(Vector File) where Lat_Long are given in ‘Reference Location’ column of 
the table listing dataset. 
5. Keep your results in a Folder and submit it in compressed format as 
PS10_[DD-MMM-YYYY]_[Startup/Group Name without Space].zip. 
6. Results will be evaluated against reference labels of the corresponding 
dataset with the organizers. 
7. Additionally, the hash value of the model needs to be generated using MD5 
hashing algorithm in text file and submitted on website on 31st Oct 2025 
along with the results of Shortlisting Dataset (online). The hash value of the 
model must be submitted along with the results of Shortlisting dataset. 
8. During offline evaluation, the hash value of the model will be verified. 
B. Offline Solution Evaluation after Stage-1 Deadline 
a. Selected participants will be asked to demonstrate their capability offline at 
IIT Delhi. 
5 
b. Participants will be allotted the slots in which they need to run their solution 
on reference data provided by the organizers on organizer’s resources with 
following specifications: - 
a. OS – Ubuntu 24.04 LTS 
b. CPU – 48+ core  
c. RAM – 256+ GB 
d. GPU -  40 GB 
e. 2 Hours 
c. During offline evaluation, the hash value of the model file will be generated and 
compared with the previous submission during online stage, to check for 
integrity and verification for nil changes using this Hash value at offline stage, 
subsequent upon which further evaluation will be carried out.  
d. Based on the results from solution demonstration and presentation, final 
scores will be computed based on Evaluation Metrics as mentioned below: 
Category 
Criteria 
Description 
Solution 
Evaluation 
Jaccard Score 
% Weight 
50 
Score based on official metric on 
hidden hold-out satellite imagery 
pair 
Resource 
Utilization 
Inference Time 
and 
Solution Memory 
Footprint  
15 
Solution Execution time on test 
dataset and 
Memory used by Solution during 
execution 
Problem 
Understandi
 ng and  Team 
Capabilities 
Start-up 
Capability 
understand 
problem 
Technical 
Capabilities 
to 
the 
and 
Start-up need to understand the 
problem and challenges for 
development of the solution. They 
will 
also 
be evaluated on 
25 
parameters 
of 
Start-up Team 
Composition, 
like 
Team 
Qualifications, 
Experience and ability to complete 
the challenge end to end. 
Approach 
Methodologies of 
Solution 
Development  
Start-up need to present Solution 
development approaches & 
proposed Architecture/ 
Innovation. 
10 
e. Top 6 teams will be selected for Stage-2, based on final score of offline 
evaluation.  
9. Evaluation Criteria for Stage-II is mentioned below: 
a. Selected participants will be provided details of dataset source and the 
object/feature classes as mentioned in Stage-2 under Section 4 – Datasets. 
6 
b. Stage-II evaluation will be based on the results from solution demonstration 
and presentation, final scores will be computed based on Evaluation Metrics 
as mentioned below: 
Category 
Criteria 
Description 
Metric 
Evaluation 
Confusion 
Matrix/Jaccard 
Index metric/ F1 
Score 
% Weight 
50 
Score based on official 
metric on hidden hold-out 
satellite imagery pair 
Resource 
Utilization 
Inference Time 
and Solution 
Memory Footprint 
30 
Solution Execution time on 
holdout dataset and 
Memory used by Solution 
during execution. 
Approach 
Methodologies of 
Solution 
Development  
Start-up need to present 
Solution development 
approaches & proposed 
Architecture  
20 
10. Evaluation Criteria for Stage-III will be shared with selected participants of Stage-3. It 
would be on similar lines as Stage-2. 
7 